{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to make tables and figures for the arxiv preprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rc('hatch', color='w', linewidth=1.5)\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 6.5}\n",
    "plt.rc('font', **font)\n",
    "plt.rc('hatch', color='w', linewidth=0.5)\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "plt.rcParams['xtick.major.width'] = 0.5\n",
    "plt.rcParams['ytick.major.width'] = 0.5\n",
    "from visual_utils.utils_tabfig import *\n",
    "sys.path.append('../')\n",
    "from src_safety_evaluation.validation_utils.utils_eval_metrics import *\n",
    "from src_safety_evaluation.validation_utils.utils_evaluation import optimize_threshold\n",
    "\n",
    "path_processed = '../ProcessedData/'\n",
    "path_prepared = '../PreparedData/'\n",
    "path_result = '../ResultData/'\n",
    "path_raw = '../RawData/'\n",
    "path_fig = '../../arXiv/Figures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 1 Accurate detection of safety-critical events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['highD_current', 'ACT', 'TTC2D', 'TAdv', 'EI']\n",
    "model_labels = ['GSSM', 'ACT', 'TTC2D', 'TAdv', 'EI']\n",
    "colors = cmap([0.15, 0.3, 0.45, 0.6, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_files = os.listdir(path_result + 'Conflicts/Results/')\n",
    "warning_files = [f for f in warning_files if f.startswith('RiskEval_') and f.endswith('.h5')]\n",
    "conflict_warning = pd.concat([pd.read_hdf(path_result+'Conflicts/Results/'+f, key='results') for f in tqdm(warning_files, desc='Reading files')])\n",
    "voted_events = pd.read_csv(path_result + 'Conflicts/Voted_conflicting_targets.csv').set_index('event_id')\n",
    "voted_events = voted_events[voted_events['target_id']>=0]\n",
    "voted_events['event'] = [category[c] for c in voted_events['event_category'].values]\n",
    "len(voted_events), voted_events['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(4.5, 1.8), constrained_layout=True)\n",
    "\n",
    "## Receiver operating characteristic curves\n",
    "ax_roc = axes[0]\n",
    "ax_roc.set_title('Receiver operating characteristic\\ncurve (ROC)', pad=5)\n",
    "ax_roc.set_aspect('equal')\n",
    "ax_roc.fill_between([0,80], 80, 100, fc=light_color('tab:red',0.05), ec=light_color('tab:red',0.35), lw=0.35, zorder=-10, label='Safety-critical')\n",
    "event_curve(ax_roc, models, colors, conflict_warning, curve_type='roc')\n",
    "ax_roc.set_xlim(0, 80)\n",
    "ax_roc.set_ylim(20, 100)\n",
    "ax_roc.tick_params(axis='both', which='both', pad=2, direction='in')\n",
    "ax_roc.set_xticks([0, 20, 40, 60, 80])\n",
    "ax_roc.set_yticks([20, 40, 60, 80, 100])\n",
    "\n",
    "## Precision-recall curves\n",
    "ax_prc = axes[1]\n",
    "ax_prc.set_title('Precision-recall curve\\n(PRC)', pad=5)\n",
    "ax_prc.set_aspect('equal')\n",
    "ax_prc.fill_betweenx([20,100], 80, 100, fc=light_color('tab:red',0.05), ec=light_color('tab:red',0.35), lw=0.35, zorder=-10)\n",
    "event_curve(ax_prc, models, colors, conflict_warning, curve_type='prc')\n",
    "ax_prc.tick_params(axis='both', which='both', pad=2, direction='in')\n",
    "ax_prc.set_xlim(20, 100)\n",
    "ax_prc.set_ylim(20, 100)\n",
    "ax_prc.set_xticks([40, 60, 80, 100])\n",
    "ax_prc.set_yticks([20, 40, 60, 80, 100])\n",
    "\n",
    "## Time to alert curves\n",
    "ax_time = axes[2]\n",
    "ax_time.set_title('Accuracy-timeliness curve\\n(ATC)', pad=5)\n",
    "ax_time.set_xlim(0.5, 4.5)\n",
    "ax_time.set_ylim(0.425, 0.925)\n",
    "ax_time.set_aspect(4./0.5)\n",
    "ax_time.fill_between([0.5,4.5], 0.8, 0.925, fc=light_color('tab:red',0.05), ec=light_color('tab:red',0.35), lw=0.35, zorder=-10)\n",
    "event_curve(ax_time, models, colors, conflict_warning, curve_type='atc')\n",
    "ax_time.tick_params(axis='both', which='both', pad=2, direction='in')\n",
    "\n",
    "handles, legends = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, ['Safety-critical']+model_labels, loc='lower center', bbox_to_anchor=(0.5, -0.05),\n",
    "           ncol=len(models)+1, frameon=False, handlelength=2.5, handletextpad=0.4, columnspacing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(path_fig + 'Result1.pdf', dpi=600, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table = pd.DataFrame(columns=['model','auprc','aroc_80','aroc_90','pprc_80','pprc_90','PTTI_star','mTTI_star'])\n",
    "for model, model_label in zip(models, model_labels):\n",
    "    metrics = get_eval_metrics(conflict_warning[conflict_warning['model']==model],\n",
    "                               thresholds={'roc': [0.80, 0.90], 'prc':[0.80, 0.90],'tti':1.5})\n",
    "    metrics['model'] = model_label\n",
    "    metric_table.loc[len(metric_table), list(metrics.keys())] = list(metrics.values())\n",
    "metric_table[metric_table.columns[1:]] = metric_table[metric_table.columns[1:]].astype(float)\n",
    "metric_table = highlight(metric_table, max_cols=metric_table.columns[1:], involve_second=True)\n",
    "metric_table = metric_table.rename(columns={'model':'Method', 'auprc':'AUPRC',\n",
    "                             'aroc_80':'$A_{80\\\\%}^\\\\mathrm{ROC}$', 'aroc_90':'$A_{90\\\\%}^\\\\mathrm{ROC}$',\n",
    "                             'pprc_80':'$\\\\mathrm{Precision}_{80\\\\%}^\\\\mathrm{PRC}$', 'pprc_90':'$\\\\mathrm{Precision}_{90\\\\%}^\\\\mathrm{PRC}$',\n",
    "                             'PTTI_star':'$P^*_{\\\\mathrm{TTI}\\\\geq1.5}$','mTTI_star':'$m\\\\mathrm{TTI}^*$'})\n",
    "metric_table = metric_table.set_index('Method').loc[model_labels]\n",
    "metric_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 2 Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_files = os.listdir(path_result + 'Analyses/')\n",
    "warning_files = [f for f in warning_files if f.startswith('Warning_') and f.endswith('.h5')]\n",
    "warning_files = [f for f in warning_files if 'mixed' in f or 'SafeBaseline' in f]\n",
    "conflict_warning = pd.concat([pd.read_hdf(path_result+'Analyses/'+f, key='results') for f in tqdm(warning_files, desc='Reading files')])\n",
    "event_meta = pd.read_csv(path_result + 'Analyses/EventMeta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = draw_data_scalability(conflict_warning, event_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(path_fig + 'Result2.pdf', dpi=600, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 3 Context-awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_files = os.listdir(path_result + 'Conflicts/Results/')\n",
    "warning_files = [f for f in warning_files if f.startswith('RiskEval_') and f.endswith('.h5')]\n",
    "conflict_warning = pd.concat([pd.read_hdf(path_result+'Conflicts/Results/'+f, key='results') for f in tqdm(warning_files, desc='Reading files')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = draw_feature_scalability(conflict_warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(path_fig + 'Result3.pdf', dpi=600, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 4 Generalisability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_warning['model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_files = os.listdir(path_result + 'Conflicts/Results/')\n",
    "warning_files = [f for f in warning_files if f.startswith('RiskEval_') and f.endswith('.h5')]\n",
    "conflict_warning = pd.concat([pd.read_hdf(path_result+'Conflicts/Results/'+f, key='results') for f in tqdm(warning_files, desc='Reading files')])\n",
    "event_meta = pd.read_csv(path_result + 'Analyses/EventMeta.csv')\n",
    "\n",
    "models = ['SafeBaseline_current_environment_profiles', \n",
    "          'ACT', \n",
    "          'TTC2D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = draw_generalisability(conflict_warning, models, event_meta, voted_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(path_fig + 'Result4.pdf', dpi=600, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_meta = event_meta[event_meta['conflict']!='none']\n",
    "model_labels = ['GSSM', 'ACT', 'TTC2D', 'TAdv', 'EI']\n",
    "\n",
    "event_metric_list = []\n",
    "event_metric_list.append([['leading'], 'Rear-end'])\n",
    "event_metric_list.append([['adjacent_lane'], 'Adjacent lane'])\n",
    "event_metric_list.append([['turning_into_parallel','turning_across_opposite','turning_into_parallel','turning_into_opposite','intersection_crossing'], \n",
    "                         'Crossing/turning'])\n",
    "event_metric_list.append([['merging'], 'Merging'])\n",
    "event_metric_list.append([['pedestrian','cyclist','animal'], 'With pedestrian/cyclist/animal'])\n",
    "\n",
    "metric_table = []\n",
    "for events, event_label in tqdm(event_metric_list):\n",
    "    event_metrics = pd.DataFrame(columns=['Event', 'model','auprc','aroc_80','aroc_90','pprc_80','pprc_90','PTTI_star','mTTI_star'])\n",
    "    for model, model_label in zip(models, model_labels):\n",
    "        event_ids = event_meta[event_meta['conflict'].isin(events)]['event_id'].values\n",
    "        filtered_warning = conflict_warning[(conflict_warning['event_id'].isin(event_ids))]\n",
    "        filtered_warning = filtered_warning[filtered_warning['model']==model]\n",
    "\n",
    "        metrics = get_eval_metrics(filtered_warning, thresholds={'roc': [0.80, 0.90], 'prc':[0.80, 0.90],'tti':1.5})\n",
    "        metrics['Event'] = event_label\n",
    "        metrics['model'] = model_label\n",
    "        event_metrics.loc[len(event_metrics), list(metrics.keys())] = list(metrics.values())\n",
    "        if model_label=='S-CaET':\n",
    "            _, _, optimal_threshold = optimize_threshold(filtered_warning, 'GSSM', curve_type='PRC', return_stats=True)\n",
    "            print(f'{event_label} FP:{optimal_threshold['FP']}, FN:{optimal_threshold['FN']}')                                            \n",
    "    event_metrics[event_metrics.columns[2:]] = event_metrics[event_metrics.columns[2:]].astype(float)\n",
    "    event_metrics = highlight(event_metrics, max_cols=event_metrics.columns[2:], involve_second=True)\n",
    "    event_metrics['Number of events'] = f\"{filtered_warning['event_id'].nunique()}\"\n",
    "    metric_table.append(event_metrics)\n",
    "metric_table = pd.concat(metric_table, axis=0)\n",
    "metric_table = metric_table.rename(columns={'model':'Method', 'aroc_80':'$A_{80\\\\%}^\\\\mathrm{ROC}$', 'aroc_90':'$A_{90\\\\%}^\\\\mathrm{ROC}$',\n",
    "                                            'pprc_80':'$\\\\mathrm{Precision}_{80\\\\%}^\\\\mathrm{PRC}$', 'pprc_90':'$\\\\mathrm{Precision}_{90\\\\%}^\\\\mathrm{PRC}$',\n",
    "                                            'auprc':'$\\\\mathrm{AUPRC}$', 'PTTI_star':'$P^*_{\\\\mathrm{TTI}\\\\geq1.5}$', 'mTTI_star':'$m\\\\mathrm{TTI}^*$'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table = metric_table.set_index(['Event','Number of events','Method'])\n",
    "metric_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list = [['leading'], ['merging']]\n",
    "model = models[0]\n",
    "\n",
    "for events in events_list:\n",
    "    print(events)\n",
    "    event_ids = event_meta[event_meta['conflict'].isin(events)]['event_id'].values\n",
    "    filtered_warning = conflict_warning[(conflict_warning['event_id'].isin(event_ids))]\n",
    "    filtered_warning = filtered_warning[filtered_warning['model']==model]\n",
    "    _, _, optimal_threshold = optimize_threshold(filtered_warning, 'GSSM', return_stats=True)\n",
    "    print(f'FP:{optimal_threshold['FP']}/({optimal_threshold['TP']+optimal_threshold['FP']}) FN:{optimal_threshold['FN']}')                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 5 Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution = pd.read_hdf(path_result + 'FeatureAttribution/SafeBaseline_current_environment_profiles.h5').reset_index()\n",
    "eg_columns = [var for var in attribution.columns[4:25]]\n",
    "columns = [var[3:] for var in attribution.columns[4:25]]\n",
    "attribution['eg_sum'] = attribution[eg_columns].sum(axis=1)\n",
    "positive_mask = (attribution[eg_columns]>0)\n",
    "attribution['positive_sum'] = (attribution[eg_columns]*positive_mask.astype(int)).sum(axis=1)\n",
    "negative_mask = (attribution[eg_columns]<0)\n",
    "attribution['negative_sum'] = (attribution[eg_columns]*negative_mask.astype(int)).sum(axis=1)\n",
    "\n",
    "conflict_warning = pd.read_hdf(path_result + 'Conflicts/Results/RiskEval_SafeBaseline_current_environment_profiles.h5', key='results')\n",
    "_, _, optimal_threshold = optimize_threshold(conflict_warning, 'GSSM', return_stats=True)\n",
    "conflict_warning = conflict_warning[conflict_warning['threshold']==optimal_threshold['threshold']].set_index('event_id')\n",
    "_, _, meta_events, environment = read_meta(path_processed, path_result)\n",
    "event_ids = conflict_warning.index.values\n",
    "meta_events = meta_events.loc[event_ids]\n",
    "environment = environment.loc[event_ids]\n",
    "\n",
    "def get_rank(conflict_warning, attribution, optimal_threshold, type='both'):\n",
    "    warning_attribution = []\n",
    "    non_warning_attribution = []\n",
    "    for event_id in tqdm(conflict_warning.index.values):\n",
    "        start_time, end_time = conflict_warning.loc[event_id][['danger_start','danger_end']].values/1000\n",
    "        if conflict_warning.loc[event_id]['true_warning']>0.5:\n",
    "            wattr = attribution[(attribution['event_id']==event_id)&\n",
    "                                (attribution['time']>=start_time)&(attribution['time']<=end_time)]\n",
    "            warning_attribution.append(wattr)\n",
    "        if conflict_warning.loc[event_id]['num_true_non_warning']>0.5:\n",
    "            nattr = attribution[(attribution['event_id']==event_id)&\n",
    "                                (attribution['time']<start_time-3)]\n",
    "            non_warning_attribution.append(nattr)\n",
    "    warning_attribution = pd.concat(warning_attribution).reset_index(drop=True)\n",
    "    non_warning_attribution = pd.concat(non_warning_attribution).reset_index(drop=True)\n",
    "\n",
    "    if type=='non_warning' or type=='both':\n",
    "        non_warning_attribution = non_warning_attribution[non_warning_attribution['intensity']<=optimal_threshold['threshold']]\n",
    "        non_warning_statistics = pd.DataFrame(np.zeros((1,len(eg_columns))), columns=eg_columns)\n",
    "        for idx in tqdm(range(len(non_warning_attribution)), desc='Non-warning attribution'):\n",
    "            attrs = non_warning_attribution.iloc[idx][eg_columns]\n",
    "            if np.all(attrs>=0):\n",
    "                continue\n",
    "            top3 = attrs[attrs<0].nsmallest(3)\n",
    "            non_warning_statistics.loc[0,top3.index.values] = non_warning_statistics.loc[0,top3.index.values] + 1 #top3.values\n",
    "        non_warning_statistics = non_warning_statistics.loc[0]\n",
    "    else:\n",
    "        non_warning_statistics = None\n",
    "\n",
    "    if type=='warning' or type=='both':\n",
    "        warning_attribution = warning_attribution[warning_attribution['intensity']>optimal_threshold['threshold']]\n",
    "        warning_statistics = pd.DataFrame(np.zeros((1,len(eg_columns))), columns=eg_columns)\n",
    "        for idx in tqdm(range(len(warning_attribution)), desc='Warning attribution'):\n",
    "            attrs = warning_attribution.iloc[idx][eg_columns]\n",
    "            if np.all(attrs<=0):\n",
    "                continue\n",
    "            top3 = attrs[attrs>0].nlargest(3)\n",
    "            warning_statistics.loc[0,top3.index.values] = warning_statistics.loc[0,top3.index.values] + 1 #top3.values\n",
    "        warning_statistics = warning_statistics.loc[0]\n",
    "    else:\n",
    "        warning_statistics = None\n",
    "    \n",
    "    return warning_statistics, non_warning_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(7.05, 3.), constrained_layout=True, gridspec_kw={'wspace':0.1})\n",
    "def settle_ax(ax):\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    ax.tick_params(axis='both', which='both', direction='in')\n",
    "    ax.set_yticks([ax.get_ylim()[0], ax.get_ylim()[1]])\n",
    "    ax.set_yticklabels([])\n",
    "    xmax = ax.get_xlim()[1]\n",
    "    xticks = ax.get_xticks()\n",
    "    if xticks[-1]>xmax:\n",
    "        xticks = list(xticks[:-1])\n",
    "        xticklabels = ax.get_xticklabels()[:-1]\n",
    "        if xmax-xticks[-1]<xmax*0.02:\n",
    "            ax.set_xticks(xticks)\n",
    "            ax.set_xticklabels(xticklabels)\n",
    "        else:\n",
    "            ax.set_xticks(xticks+[xmax])\n",
    "            ax.set_xticklabels(xticklabels+[''])\n",
    "\n",
    "def get_sorted(statistics, proportion=0.9, number=None):\n",
    "    statistics = statistics/statistics.sum()\n",
    "    statistics = statistics.sort_values(ascending=False)\n",
    "    if number is None:\n",
    "        statistics = statistics[statistics.cumsum()<proportion]\n",
    "    else:\n",
    "        statistics = statistics[:number]\n",
    "    return statistics\n",
    "\n",
    "def plot_bars(statistics, ax):\n",
    "    stat2plot = get_sorted(statistics, proportion=0.9, number=6)\n",
    "    ax.barh(np.arange(len(stat2plot)), stat2plot.values[::-1],\n",
    "            color=cmap(np.linspace(0.65, 0.15, len(stat2plot))), alpha=0.75, lw=0.35)\n",
    "    xmax = ax.get_xlim()[1]\n",
    "    for pos, label in zip(np.arange(len(stat2plot)), stat2plot.index[::-1]):\n",
    "        if label=='eg_Sur lat speed':\n",
    "            label = \"eg_Surrounding object's lateral speed\"\n",
    "        if label=='eg_Sur lon speed':\n",
    "            label = \"eg_Surrounding object's longitudinal speed\"\n",
    "        if label=='eg_2D spacing direction':\n",
    "            label = 'eg_Spacing direction'\n",
    "        ax.text(xmax*0.99, pos, label.split('_')[1], ha='right', va='center', color='k')\n",
    "    settle_ax(ax)\n",
    "\n",
    "\n",
    "ax_conflit = axes[0]\n",
    "ax_conflit.set_title('(a) Top factors in leading to and avoiding lateral conflicts', pad=15)\n",
    "remove_box(ax_conflit)\n",
    "ax_adj_danger = ax_conflit.inset_axes([0, 0.55, 0.45, 0.45])\n",
    "ax_adj_danger.set_title('Danger in adjacent lane', pad=3)\n",
    "filtered_warning = conflict_warning.loc[meta_events[(meta_events['conflict']=='Adjacent lane')].index.values]\n",
    "warning_statistics, non_warning_statistics = get_rank(filtered_warning, attribution, optimal_threshold, type='both')\n",
    "plot_bars(warning_statistics, ax_adj_danger)\n",
    "\n",
    "ax_adj_safe = ax_conflit.inset_axes([0.55, 0.55, 0.45, 0.45])\n",
    "ax_adj_safe.set_title('Safe in adjacent lane', pad=3)\n",
    "plot_bars(non_warning_statistics, ax_adj_safe)\n",
    "\n",
    "ax_cat_danger = ax_conflit.inset_axes([0, -0.05, 0.45, 0.45])\n",
    "ax_cat_danger.set_title('Danger during crossing/turning', pad=3)\n",
    "filtered_warning = conflict_warning.loc[meta_events[(meta_events['conflict']=='Crossing/turning')].index.values]\n",
    "warning_statistics, non_warning_statistics = get_rank(filtered_warning, attribution, optimal_threshold, type='both')\n",
    "plot_bars(warning_statistics, ax_cat_danger)\n",
    "\n",
    "ax_cat_safe = ax_conflit.inset_axes([0.55, -0.05, 0.45, 0.45])\n",
    "ax_cat_safe.set_title('Safe during crossing/turning', pad=3)\n",
    "plot_bars(non_warning_statistics, ax_cat_safe)\n",
    "\n",
    "ax_environment = axes[1]\n",
    "ax_environment.set_title('(b) Top factors in conflicts in adverse environments', pad=15)\n",
    "remove_box(ax_environment)\n",
    "ax_wea = ax_environment.inset_axes([0, 0.55, 0.45, 0.45])\n",
    "ax_wea.set_title('Raining weather', pad=3)\n",
    "filtered_warning = conflict_warning.loc[environment[(environment['weather']=='Raining')|\n",
    "                                                    (environment['weather']=='Mist/Light Rain')].index.values]\n",
    "warning_statistics, _ = get_rank(filtered_warning, attribution, optimal_threshold, type='warning')\n",
    "plot_bars(warning_statistics, ax_wea)\n",
    "\n",
    "ax_road = ax_environment.inset_axes([0.55, 0.55, 0.45, 0.45])\n",
    "ax_road.set_title('Not dry road', pad=3)\n",
    "filtered_warning = conflict_warning.loc[environment[(environment['surfaceCondition']!='Dry')].index.values]\n",
    "warning_statistics, _ = get_rank(filtered_warning, attribution, optimal_threshold, type='warning')\n",
    "plot_bars(warning_statistics, ax_road)\n",
    "\n",
    "ax_light = ax_environment.inset_axes([0, -0.05, 0.45, 0.45])\n",
    "ax_light.set_title('Not in daylight', pad=3)\n",
    "filtered_warning = conflict_warning.loc[environment[(environment['lighting']!='Daylight')].index.values]\n",
    "warning_statistics, _ = get_rank(filtered_warning, attribution, optimal_threshold, type='warning')\n",
    "plot_bars(warning_statistics, ax_light)\n",
    "\n",
    "ax_traffic = ax_environment.inset_axes([0.55, -0.05, 0.45, 0.45])\n",
    "ax_traffic.set_title('LOS D Unstable flow', pad=3)\n",
    "filtered_warning = conflict_warning.loc[environment[(environment['trafficDensity']=='Level-of-service D: Unstable flow - temporary restrictions substantially slow driver')].index.values]\n",
    "warning_statistics, _ = get_rank(filtered_warning, attribution, optimal_threshold, type='warning')\n",
    "plot_bars(warning_statistics, ax_traffic)\n",
    "\n",
    "for ax in [ax_cat_danger, ax_cat_safe, ax_light, ax_traffic]:\n",
    "    ax.set_xlabel('Frequency of being top 3 factors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(path_fig + 'Result5.pdf', dpi=600, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure A1 SHRP2 Reconstruction error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_both = pd.read_csv(path_processed + 'SHRP2/metadata_birdseye.csv').set_index('event_id')\n",
    "meta_both['event'] = [category[c] for c in meta_both['event_category'].values]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(7.05, 1.8), constrained_layout=True)\n",
    "\n",
    "var_list = {'Subject speed': ['m/s', 'v_ekf','speed_comp', np.linspace(-0.008, 0.408, 30)],\n",
    "            'Subject yaw rate': ['rad/s', 'omega_ekf','yaw_rate', np.linspace(-0.00004, 0.00204, 30)],\n",
    "            'Subject acceleration': ['m/s$^2$', 'acc_ekf','acc_lon', np.linspace(-0.02, 1.02, 30)],\n",
    "            'Object displacement': ['m', np.linspace(-0.04, 2.04, 30)],\n",
    "            'Object speed': ['m/s', 'v_ekf','speed_comp', np.linspace(-0.02, 1.02, 30)]}\n",
    "for event_type, color, text_pos in zip(['Crashes', 'Near-crashes', 'Safe baselines'], [cmap(0.4), cmap(0.65), cmap(0.15)], [0.95, 0.65, 0.35]):\n",
    "    if event_type == 'Safe baselines':\n",
    "        data_ego = pd.concat([pd.read_hdf(path_processed + f'SHRP2/SafeBaseline/Ego_birdseye_{i}.h5', key='data') for i in range(1, 5)])\n",
    "        data_sur = pd.concat([pd.read_hdf(path_processed + f'SHRP2/SafeBaseline/Surrounding_birdseye_{i}.h5', key='data') for i in range(1, 5)])\n",
    "    else:\n",
    "        event_categories = meta_both[meta_both['event']==event_type]['event_category'].unique()\n",
    "        data_ego = []\n",
    "        data_sur = []\n",
    "        for event_cat in event_categories:\n",
    "            data_ego.append(pd.read_hdf(path_processed + f'SHRP2/{event_cat}/Ego_birdseye.h5', key='data'))\n",
    "            data_sur.append(pd.read_hdf(path_processed + f'SHRP2/{event_cat}/Surrounding_birdseye.h5', key='data'))\n",
    "        data_ego = pd.concat(data_ego)\n",
    "        data_sur = pd.concat(data_sur)\n",
    "    \n",
    "    data_list = [data_ego, data_ego, data_ego, data_sur, data_sur]\n",
    "    for col, data, key, values in zip(range(5), data_list, var_list.keys(), var_list.values()):\n",
    "        if key == 'Object displacement':\n",
    "            # Mean displacement error\n",
    "            error = ((data['x_ekf']-data['x'])**2 + (data['y_ekf']-data['y'])**2)**0.5\n",
    "            error = error.to_frame(name='error')\n",
    "            error['event_id'] = data['event_id']\n",
    "            error = error.groupby('event_id')['error'].mean()\n",
    "        else:\n",
    "            # Root mean square error\n",
    "            error = (data[values[1]]-data[values[2]])**2\n",
    "            error = error.to_frame(name='squared error')\n",
    "            error['event_id'] = data['event_id']\n",
    "            error = error.groupby('event_id')['squared error'].mean()**0.5\n",
    "            \n",
    "        mean, std = error.mean(), error.std()\n",
    "        axes[col].hist(error, bins=values[-1], density=True, color=color, alpha=0.6, lw=0, label=event_type)\n",
    "        if key == 'Subject yaw rate':\n",
    "            axes[col].text(0.6, text_pos, f'$\\\\mu={mean:.5f}$\\n$\\\\sigma={std:.5f}$', ha='left', va='top', transform=axes[col].transAxes, color=color)\n",
    "        else:\n",
    "            axes[col].text(0.7, text_pos, f'$\\\\mu={mean:.2f}$\\n$\\\\sigma={std:.2f}$', ha='left', va='top', transform=axes[col].transAxes, color=color)\n",
    "        axes[col].set_title(f'{key}')\n",
    "        if key == 'Object displacement':\n",
    "            axes[col].set_xlabel(f'{key.split(' ')[1].capitalize()} MAE ({values[0]})', labelpad=1)\n",
    "        else:\n",
    "            axes[col].set_xlabel(f'{key.split(' ')[1].capitalize()} RMSE ({values[0]})', labelpad=1)\n",
    "        axes[col].set_yticks([])\n",
    "axes[0].set_ylabel('Probability density')\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles[:3], labels[:3], loc='lower center', bbox_to_anchor=(0.5, -0.1),\n",
    "           ncol=3, frameon=False, handlelength=1, handletextpad=0.5, columnspacing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(path_fig + 'SHRP2_error_distributions.pdf', dpi=600, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conflict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
